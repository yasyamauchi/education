{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasyamauchi/education/blob/main/notebooks/05.07-Support-Vector-Machines_BME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# サポートベクタマシン (Support Vector Machine, SVM)  \n",
        "  \n",
        "[In-Depth: Support Vector Machines](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html), J. VanderPlasより"
      ],
      "metadata": {
        "id": "Tz74_qJ9f1iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SMV)は分類と予測で用いられる有名なアルゴリズムである．  \n",
        "この例ではscikit-learnのSVMを使用する．  \n",
        "まず必要なライブラリをインポートする．"
      ],
      "metadata": {
        "id": "FsID-5V7gtIa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "nbEXyawnTUCk"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g06j-ageTUCl"
      },
      "source": [
        "## 序論"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずは単純に直線や曲線で分類する．例として，2つのクラスの点が十分に離れている単純なケースを考える．  \n",
        "データはsklearnで生成する．"
      ],
      "metadata": {
        "id": "dvFxTRuKhZC8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ESi6kyhdTUCl"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1つの線(識別境界)で2つのデータセットを分類することができるが，そのような線は無数にあることが分かる．  \n",
        "いずれにせよ，識別境界が決まったら，新しいデータ(ここでは×)について分類しラベルを与えることができる．\n",
        "  \n",
        "しかし，これで終わりではなく，もうちょっと深く考えてみる．"
      ],
      "metadata": {
        "id": "IP3ky29chpRG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ANCRJNTCTUCm"
      },
      "outputs": [],
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwGj-FP-TUCn"
      },
      "source": [
        "## マージンを最大化する"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVMは，これを改善するために，異なるクラス間の距離（マージン）を最大化させる．  \n",
        "マージンとは下の図の灰色の領域となる．これが一番広いものがベストとなる．"
      ],
      "metadata": {
        "id": "98kOz4ebiTS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "r6lGQrMeTUCn"
      },
      "outputs": [],
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='lightgray', alpha=0.5)\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d42Hq8NTUCn"
      },
      "source": [
        "### フィッティング\n",
        "\n",
        "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier (`SVC`) to train an SVM model on this data.\n",
        "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際にSVMを適用してみる．"
      ],
      "metadata": {
        "id": "psyaKK_Wiljr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bCmexcEaTUCn"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-sdU7JbKTUCo"
      },
      "outputs": [],
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "\n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "\n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, edgecolors='black',\n",
        "                   facecolors='none');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "h1HEt-E-TUCo"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "求めた識別境界が表示されたはずである．いくつかの(3つ？)のデータはちょうどマージンに接していることがわかる(黒丸で囲った点)．これらを「サポートベクトル」とよぶ．  \n",
        "  \n",
        "サポートベクトルの座標を表示してみる．"
      ],
      "metadata": {
        "id": "urbHPUViiu0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JrdNebkYTUCo"
      },
      "outputs": [],
      "source": [
        "model.support_vectors_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このSVMの特徴は，マージンはサポートベクトル「のみ」によって決まることにある．マージンを超えない限り，データの数は重要ではない．  \n",
        "次の図ではデータの最初の60個と120個から学習したものであるが，3つのサポートベクトルが同じである．"
      ],
      "metadata": {
        "id": "NLaMlyrAjR-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "EIQTv1D8TUCo"
      },
      "outputs": [],
      "source": [
        "def plot_svm(N=10, ax=None):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    model = SVC(kernel='linear', C=1E10)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    ax = ax or plt.gca()\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(model, ax)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [60, 120]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次の例ではスライダーを操作することによって任意のデータ数から得られたモデルを表示することができる．"
      ],
      "metadata": {
        "id": "Ew1cJZFUjzB6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "sVhqtLhpTUCp"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact, fixed\n",
        "interact(plot_svm, N=(10, 200), ax=fixed(None));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o8ulJZITUCp"
      },
      "source": [
        "### カーネルSVM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "分離境界が直線である線形SVMが必ずしもうまくいかない場合，「カーネルSVM」という方法がある．  \n",
        "実際に線形分離ができない例を見る．"
      ],
      "metadata": {
        "id": "qQ0XjAtHkf8i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JYA38WWJTUCp"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf, plot_support=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "この例はどのような直線の分離境界でもデータを分離することができない．しかし，「カーネル関数(核関数)」というものを使い，2次元のデータを3次元上に投影する(次元拡張)ことで，直線で分離することが可能となる．  \n",
        "  \n",
        "  次の例では，カーネル関数として放射基底関数$r=e^{-X^2}$を用いている．これは中央が円状に盛り上がった関数である．"
      ],
      "metadata": {
        "id": "p_24dF3Jk-mk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "h9u37SE3TUCp"
      },
      "outputs": [],
      "source": [
        "r = np.exp(-(X ** 2).sum(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "三次元グラフにするとカーネル関数がどのように適用されたかが分かりやすい．"
      ],
      "metadata": {
        "id": "_LvFrJOMmbDW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Ex1KJ1RlTUCp"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
        "ax.view_init(elev=20, azim=30)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('r');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "もう明らかであるが，高さ方向(r軸)に分離「平面」を置くことで，データが分離できる．  \n",
        "  \n",
        "しかし，問題はカーネル関数をどのように決めるかである．まず，放射基底関数の中心が正しい位置にある必要がある．また，そもそも単純な放射基底関数でうまくいくとは限らない．  \n",
        "  \n",
        "「カーネルSVM」とは，この問題を解決するため，いったんすべてのデータを中心とする放射基底関数を設けて計算し，その中でアルゴリズムに選択させる方法である．"
      ],
      "metadata": {
        "id": "AFkQz8C_mkIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "カーネルSVMを適用し結果を見てみる．今回はカーネル関数の形のみ(rbf:放射基底関数)を指定している．"
      ],
      "metadata": {
        "id": "0gd8E9OVqTlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "j2Mj5G3ATUCp"
      },
      "outputs": [],
      "source": [
        "clf = SVC(kernel='rbf', C=1E6)\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6cIjh-FTUCq"
      },
      "source": [
        "Let's use our previously defined function to visualize the fit and identify the support vectors (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "FlNlQWmmTUCq"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=300, lw=1, facecolors='none');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcC1raePTUCq"
      },
      "source": [
        "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
        "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEZNP9LyTUCq"
      },
      "source": [
        "### ソフトマージン"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2つのクラスのデータがどうしてもオーバーラップする場合はどうすればよいか？  \n",
        "例を見てみる．"
      ],
      "metadata": {
        "id": "5QMufPveq3vA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "iOb3UOBgTUCq"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=1.2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "こういうケースでは完全なマージンを見つけることはできないので，ある程度のはみだしを許容する必要がある．このとき，どの程度入り込むことまで許容するか(硬いか，柔らかいか)を定めることができる．  \n",
        "次の例では，パラメータCが大きいと「硬く」，小さいと「柔らかい」．"
      ],
      "metadata": {
        "id": "vQQNeclJrFip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "vJvsX3jBTUCq"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=0.8)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for axi, C in zip(ax, [10.0, 0.1]):\n",
        "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
        "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    plot_svc_decision_function(model, axi)\n",
        "    axi.scatter(model.support_vectors_[:, 0],\n",
        "                model.support_vectors_[:, 1],\n",
        "                s=300, lw=1, facecolors='none');\n",
        "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_mVwqkSTUCw"
      },
      "source": [
        "The optimal value of `C` will depend on your dataset, and you should tune this parameter using cross-validation or a similar procedure (refer back to [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QClXbsLXTUCw"
      },
      "source": [
        "## 例：顔の認識"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "顔画像認識の例  \n",
        "scikit-learnの*Wild*という顔画像データセットを用いる．これは著名人の顔画像のデータである．  \n",
        "まずデータを読み込む．"
      ],
      "metadata": {
        "id": "2JxUJ11irqvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "rTGHrB7hTUCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ちょっと画像を見てみる．  \n",
        "各国の首脳や政治家の画像である．"
      ],
      "metadata": {
        "id": "yR1CkDH5sDkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "VenaL871TUCw"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(3, 5, figsize=(8, 6))\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(faces.images[i], cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[],\n",
        "            xlabel=faces.target_names[faces.target[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "各画像は62x47ピクセルである．今回はこのピクセル値をそのままではなく，主成分分析により150個の基本成分に抽出したものを用いる．"
      ],
      "metadata": {
        "id": "UUj7onm_sNok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WHecUzRkTUCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pca = PCA(n_components=150, whiten=True,\n",
        "          svd_solver='randomized', random_state=42)\n",
        "svc = SVC(kernel='rbf', class_weight='balanced')\n",
        "model = make_pipeline(pca, svc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "例によって訓練データと検証データに分ける．"
      ],
      "metadata": {
        "id": "ubvesjoNsn2r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "nm5cJp6UTUCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
        "                                                random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "マージンの硬さ(C)と放射基底関数のカーネルサイズ(gamma)をいくつか用意し，最良のものを求める．  \n",
        "結果としてC=5，gamma=.001の時に最良であることがわかる．  \n",
        "**数分かかる**"
      ],
      "metadata": {
        "id": "P0exNbAXsvmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "xEsO-eY3TUCx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'svc__C': [1, 5, 10, 50],\n",
        "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
        "grid = GridSearchCV(model, param_grid)\n",
        "\n",
        "%time grid.fit(Xtrain, ytrain)\n",
        "print(grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "検証データで検証する．いくつかの検証データのサンプルの分類結果を表示する．名前が赤くなっているものが，間違った分類となる．"
      ],
      "metadata": {
        "id": "SQBjztaNtj4x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yvslZ9nFTUCx"
      },
      "outputs": [],
      "source": [
        "model = grid.best_estimator_\n",
        "yfit = model.predict(Xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "gX8oCIgiTUCx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(4, 6)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
        "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
        "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "（表示される間違ったデータは実行ごとに異なる）"
      ],
      "metadata": {
        "id": "UP5FWykvt8WV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果を統計データとして表示する．"
      ],
      "metadata": {
        "id": "v5i6dYYtuFey"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "y_DZ_hP-TUCx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, yfit,\n",
        "                            target_names=faces.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "混同行列にする．"
      ],
      "metadata": {
        "id": "Tqm7RtDZuIst"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "XSePyeaVTUCx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "mat = confusion_matrix(ytest, yfit)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues',\n",
        "            xticklabels=faces.target_names,\n",
        "            yticklabels=faces.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}