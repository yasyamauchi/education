{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasyamauchi/education/blob/main/notebooks/05.07-Support-Vector-Machines_BME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvyfLB5sTUCh"
      },
      "source": [
        "# In Depth: Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# サポートベクタマシン (Support Vector Machine, SVM)  \n",
        "  \n",
        "[In-Depth: Support Vector Machines](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html), J. VanderPlasより"
      ],
      "metadata": {
        "id": "Tz74_qJ9f1iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SMV)は分類と予測で用いられる有名なアルゴリズムである．  \n",
        "この例ではscikit-learnのSVMを使用する．  \n",
        "まず必要なライブラリをインポートする．"
      ],
      "metadata": {
        "id": "FsID-5V7gtIa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GPMf-r1TUCj"
      },
      "source": [
        "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n",
        "In this chapter, we will explore the intuition behind SVMs and their use in classification problems.\n",
        "\n",
        "We begin with the standard imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "nbEXyawnTUCk"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g06j-ageTUCl"
      },
      "source": [
        "## Motivating Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずは単純に直線や曲線で分類する．例として，2つのクラスの点が十分に離れている単純なケースを考える．  \n",
        "データはsklearnで生成する．"
      ],
      "metadata": {
        "id": "dvFxTRuKhZC8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHz-7tI0TUCl"
      },
      "source": [
        "As part of our discussion of Bayesian classification (see [In Depth: Naive Bayes Classification](05.05-Naive-Bayes.ipynb)), we learned about a simple kind of model that describes the distribution of each underlying class, and experimented with using it to probabilistically determine labels for new points.\n",
        "That was an example of *generative classification*; here we will consider instead *discriminative classification*. That is, rather than modeling each class, we will simply find a line or curve (in two dimensions) or manifold (in multiple dimensions) that divides the classes from each other.\n",
        "\n",
        "As an example of this, consider the simple case of a classification task in which the two classes of points are well separated (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ESi6kyhdTUCl"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1つの線(識別境界)で2つのデータセットを分類することができるが，そのような線は無数にあることが分かる．  \n",
        "いずれにせよ，識別境界が決まったら，新しいデータ(ここでは×)について分類しラベルを与えることができる．\n",
        "  \n",
        "しかし，これで終わりではなく，もうちょっと深く考えてみる．"
      ],
      "metadata": {
        "id": "IP3ky29chpRG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bghV02OuTUCm"
      },
      "source": [
        "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification.\n",
        "For two-dimensional data like that shown here, this is a task we could do by hand.\n",
        "But immediately we see a problem: there is more than one possible dividing line that can perfectly discriminate between the two classes!\n",
        "\n",
        "We can draw some of them as follows; the following figure shows the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ANCRJNTCTUCm"
      },
      "outputs": [],
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBWAfmByTUCn"
      },
      "source": [
        "These are three *very* different separators which, nevertheless, perfectly discriminate between these samples.\n",
        "Depending on which you choose, a new data point (e.g., the one marked by the \"X\" in this plot) will be assigned a different label!\n",
        "Evidently our simple intuition of \"drawing a line between classes\" is not good enough, and we need to think a bit more deeply."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwGj-FP-TUCn"
      },
      "source": [
        "## Support Vector Machines: Maximizing the Margin\n",
        "\n",
        "Support vector machines offer one way to improve on this.\n",
        "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
        "Here is an example of how this might look (see the following figure):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVMは，これを改善するために，異なるクラス間の距離（マージン）を最大化させる．  \n",
        "マージンとは下の図の灰色の領域となる．"
      ],
      "metadata": {
        "id": "98kOz4ebiTS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "r6lGQrMeTUCn"
      },
      "outputs": [],
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='lightgray', alpha=0.5)\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw5l3WM_TUCn"
      },
      "source": [
        "The line that maximizes this margin is the one we will choose as the optimal model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d42Hq8NTUCn"
      },
      "source": [
        "### Fitting a Support Vector Machine\n",
        "\n",
        "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier (`SVC`) to train an SVM model on this data.\n",
        "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際にSVMを適用してみる．"
      ],
      "metadata": {
        "id": "psyaKK_Wiljr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "bCmexcEaTUCn"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD5zf3rMTUCo"
      },
      "source": [
        "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "-sdU7JbKTUCo"
      },
      "outputs": [],
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "\n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "\n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, edgecolors='black',\n",
        "                   facecolors='none');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "h1HEt-E-TUCo"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "求めた識別境界が表示されたはずである．いくつかの(3つ？)のデータはちょうどマージンに接していることがわかる(黒丸で囲った点)．これらを「サポートベクトル」とよぶ．  \n",
        "  \n",
        "サポートベクトルの座標を表示してみる．"
      ],
      "metadata": {
        "id": "urbHPUViiu0G"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4s6oSKPTUCo"
      },
      "source": [
        "This is the dividing line that maximizes the margin between the two sets of points.\n",
        "Notice that a few of the training points just touch the margin: they are circled in the following figure.\n",
        "These points are the pivotal elements of this fit; they are known as the *support vectors*, and give the algorithm its name.\n",
        "In Scikit-Learn, the identities of these points are stored in the `support_vectors_` attribute of the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JrdNebkYTUCo"
      },
      "outputs": [],
      "source": [
        "model.support_vectors_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このSVMの特徴は，マージンはサポートベクトル「のみ」によって決まることにある．マージンを超えない限り，データの数は重要ではない．  \n",
        "次の図ではデータの最初の60個と120個から学習したものであるが，3つのサポートベクトルが同じである．"
      ],
      "metadata": {
        "id": "NLaMlyrAjR-y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejZT8pgETUCo"
      },
      "source": [
        "A key to this classifier's success is that for the fit, only the positions of the support vectors matter; any points further from the margin that are on the correct side do not modify the fit.\n",
        "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
        "\n",
        "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "EIQTv1D8TUCo"
      },
      "outputs": [],
      "source": [
        "def plot_svm(N=10, ax=None):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    model = SVC(kernel='linear', C=1E10)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    ax = ax or plt.gca()\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(model, ax)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [60, 120]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w01s58BxTUCo"
      },
      "source": [
        "In the left panel, we see the model and the support vectors for 60 training points.\n",
        "In the right panel, we have doubled the number of training points, but the model has not changed: the three support vectors in the left panel are the same as the support vectors in the right panel.\n",
        "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次の例ではスライダーを操作することによって任意のデータ数から得られたモデルを表示することができる．"
      ],
      "metadata": {
        "id": "Ew1cJZFUjzB6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2E-l8jgTUCo"
      },
      "source": [
        "If you are running this notebook live, you can use IPython's interactive widgets to view this feature of the SVM model interactively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "sVhqtLhpTUCp"
      },
      "outputs": [],
      "source": [
        "from ipywidgets import interact, fixed\n",
        "interact(plot_svm, N=(10, 200), ax=fixed(None));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o8ulJZITUCp"
      },
      "source": [
        "### Beyond Linear Boundaries: Kernel SVM\n",
        "\n",
        "Where SVM can become quite powerful is when it is combined with *kernels*.\n",
        "We have seen a version of kernels before, in the basis function regressions of [In Depth: Linear Regression](05.06-Linear-Regression.ipynb).\n",
        "There we projected our data into a higher-dimensional space defined by polynomials and Gaussian basis functions, and thereby were able to fit for nonlinear relationships with a linear classifier.\n",
        "\n",
        "In SVM models, we can use a version of the same idea.\n",
        "To motivate the need for kernels, let's look at some data that is not linearly separable (see the following figure):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "分離境界が直線である線形SVMが必ずしもうまくいかない場合，「カーネルSVM」という方法がある．  \n",
        "実際に線形分離ができない例を見る．"
      ],
      "metadata": {
        "id": "qQ0XjAtHkf8i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "JYA38WWJTUCp"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf, plot_support=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "この例はどのような直線の分離境界でもデータを分離することができない．しかし，「カーネル関数(核関数)」というものを使い，2次元のデータを3次元上に投影する(次元拡張)ことで，直線で分離することが可能となる．  \n",
        "  \n",
        "  次の例では，カーネル関数として放射基底関数$r=e^{-X^2}$を用いている．これは中央が円状に盛り上がった関数である．"
      ],
      "metadata": {
        "id": "p_24dF3Jk-mk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49REjQutTUCp"
      },
      "source": [
        "It is clear that no linear discrimination will *ever* be able to separate this data.\n",
        "But we can draw a lesson from the basis function regressions in [In Depth: Linear Regression](05.06-Linear-Regression.ipynb), and think about how we might project the data into a higher dimension such that a linear separator *would* be sufficient.\n",
        "For example, one simple projection we could use would be to compute a *radial basis function* (RBF) centered on the middle clump:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "h9u37SE3TUCp"
      },
      "outputs": [],
      "source": [
        "r = np.exp(-(X ** 2).sum(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "三次元グラフにするとカーネル関数がどのように適用されたかが分かりやすい．"
      ],
      "metadata": {
        "id": "_LvFrJOMmbDW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAD_X-8fTUCp"
      },
      "source": [
        "We can visualize this extra data dimension using a three-dimensional plot, as seen in the following figure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "Ex1KJ1RlTUCp"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
        "ax.view_init(elev=20, azim=30)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('r');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "もう明らかであるが，高さ方向(r軸)に分離「平面」を置くことで，データが分離できる．  \n",
        "  \n",
        "しかし，問題はカーネル関数をどのように決めるかである．まず，放射基底関数の中心が正しい位置にある必要がある．また，そもそも単純な放射基底関数でうまくいくとは限らない．  \n",
        "  \n",
        "「カーネルSVM」とは，この問題を解決するため，いったんすべてのデータを中心とする放射基底関数を設けて計算し，その中でアルゴリズムに選択させる方法である．"
      ],
      "metadata": {
        "id": "AFkQz8C_mkIo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsPBVY5iTUCp"
      },
      "source": [
        "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *r*=0.7.\n",
        "\n",
        "In this case we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
        "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
        "\n",
        "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
        "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
        "\n",
        "A potential problem with this strategy—projecting $N$ points into $N$ dimensions—is that it might become very computationally intensive as $N$ grows large.\n",
        "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection.\n",
        "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
        "\n",
        "In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF kernel, using the `kernel` model hyperparameter:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "カーネルSVMを適用し結果を見てみる．今回はカーネル関数の形のみ(rbf:放射基底関数)を指定している．"
      ],
      "metadata": {
        "id": "0gd8E9OVqTlm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "j2Mj5G3ATUCp"
      },
      "outputs": [],
      "source": [
        "clf = SVC(kernel='rbf', C=1E6)\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6cIjh-FTUCq"
      },
      "source": [
        "Let's use our previously defined function to visualize the fit and identify the support vectors (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "FlNlQWmmTUCq"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=300, lw=1, facecolors='none');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcC1raePTUCq"
      },
      "source": [
        "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
        "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEZNP9LyTUCq"
      },
      "source": [
        "### Tuning the SVM: Softening Margins\n",
        "\n",
        "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
        "But what if your data has some amount of overlap?\n",
        "For example, you may have data like this (see the following figure):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2つのクラスのデータがどうしてもオーバーラップする場合はどうすればよいか？  \n",
        "例を見てみる．"
      ],
      "metadata": {
        "id": "5QMufPveq3vA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "iOb3UOBgTUCq"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=1.2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "こういうケースでは完全なマージンを見つけることはできないので，ある程度のはみだしを許容する必要がある．このとき，どの程度入り込むことまで許容するか(硬いか，柔らかいか)を定めることができる．  \n",
        "次の例では，パラメータCが大きいと「硬く」，小さいと「柔らかい」．"
      ],
      "metadata": {
        "id": "vQQNeclJrFip"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj7Ug-MvTUCq"
      },
      "source": [
        "To handle this case, the SVM implementation has a bit of a fudge factor that \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
        "The hardness of the margin is controlled by a tuning parameter, most often known as `C`.\n",
        "For a very large `C`, the margin is hard, and points cannot lie in it.\n",
        "For a smaller `C`, the margin is softer and can grow to encompass some points.\n",
        "\n",
        "The plot shown in the following figure gives a visual picture of how a changing `C` affects the final fit via the softening of the margin:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "vJvsX3jBTUCq"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=0.8)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for axi, C in zip(ax, [10.0, 0.1]):\n",
        "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
        "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    plot_svc_decision_function(model, axi)\n",
        "    axi.scatter(model.support_vectors_[:, 0],\n",
        "                model.support_vectors_[:, 1],\n",
        "                s=300, lw=1, facecolors='none');\n",
        "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_mVwqkSTUCw"
      },
      "source": [
        "The optimal value of `C` will depend on your dataset, and you should tune this parameter using cross-validation or a similar procedure (refer back to [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QClXbsLXTUCw"
      },
      "source": [
        "## Example: Face Recognition\n",
        "\n",
        "As an example of support vector machines in action, let's take a look at the facial recognition problem.\n",
        "We will use the Labeled Faces in the Wild dataset, which consists of several thousand collated photos of various public figures.\n",
        "A fetcher for the dataset is built into Scikit-Learn:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "顔画像認識の例  \n",
        "scikit-learnの*Wild*という顔画像データセットを用いる．これは著名人の顔画像のデータである．  \n",
        "まずデータを読み込む．"
      ],
      "metadata": {
        "id": "2JxUJ11irqvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "rTGHrB7hTUCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ちょっと画像を見てみる．  \n",
        "各国の首脳や政治家の画像である．"
      ],
      "metadata": {
        "id": "yR1CkDH5sDkg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fhb3VEjMTUCw"
      },
      "source": [
        "Let's plot a few of these faces to see what we're working with (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "VenaL871TUCw"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(3, 5, figsize=(8, 6))\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(faces.images[i], cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[],\n",
        "            xlabel=faces.target_names[faces.target[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "各画像は62x47ピクセルである．今回はこのピクセル値をそのままではなく，主成分分析により150個の基本成分に抽出したものを用いる．"
      ],
      "metadata": {
        "id": "UUj7onm_sNok"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azodTsABTUCw"
      },
      "source": [
        "Each image contains 62 × 47, or around 3,000, pixels.\n",
        "We could proceed by simply using each pixel value as a feature, but often it is more effective to use some sort of preprocessor to extract more meaningful features; here we will use principal component analysis (see [In Depth: Principal Component Analysis](05.09-Principal-Component-Analysis.ipynb)) to extract 150 fundamental components to feed into our support vector machine classifier.\n",
        "We can do this most straightforwardly by packaging the preprocessor and the classifier into a single pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "WHecUzRkTUCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pca = PCA(n_components=150, whiten=True,\n",
        "          svd_solver='randomized', random_state=42)\n",
        "svc = SVC(kernel='rbf', class_weight='balanced')\n",
        "model = make_pipeline(pca, svc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "例によって訓練データと検証データに分ける．"
      ],
      "metadata": {
        "id": "ubvesjoNsn2r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfmStX7gTUCw"
      },
      "source": [
        "For the sake of testing our classifier output, we will split the data into a training set and a testing set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "nm5cJp6UTUCw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
        "                                                random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "マージンの硬さ(C)と放射基底関数のカーネルサイズ(gamma)をいくつか用意し，最良のものを求める．  \n",
        "結果としてC=5，gamma=.001の時に最良であることがわかる．"
      ],
      "metadata": {
        "id": "P0exNbAXsvmO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlhvO-QOTUCw"
      },
      "source": [
        "Finally, we can use grid search cross-validation to explore combinations of parameters.\n",
        "Here we will adjust ``C`` (which controls the margin hardness) and ``gamma`` (which controls the size of the radial basis function kernel), and determine the best model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "xEsO-eY3TUCx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'svc__C': [1, 5, 10, 50],\n",
        "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
        "grid = GridSearchCV(model, param_grid)\n",
        "\n",
        "%time grid.fit(Xtrain, ytrain)\n",
        "print(grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsAnS-aeTUCx"
      },
      "source": [
        "The optimal values fall toward the middle of our grid; if they fell at the edges, we would want to expand the grid to make sure we have found the true optimum.\n",
        "\n",
        "Now with this cross-validated model we can predict the labels for the test data, which the model has not yet seen:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "検証データで検証する．いくつかの検証データのサンプルの分類結果を表示する．名前が赤くなっているものが，間違った分類となる．"
      ],
      "metadata": {
        "id": "SQBjztaNtj4x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yvslZ9nFTUCx"
      },
      "outputs": [],
      "source": [
        "model = grid.best_estimator_\n",
        "yfit = model.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Mj1vTvTUCx"
      },
      "source": [
        "Let's take a look at a few of the test images along with their predicted values (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "gX8oCIgiTUCx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(4, 6)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
        "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
        "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "（表示される間違ったデータは実行ごとに異なる）"
      ],
      "metadata": {
        "id": "UP5FWykvt8WV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "結果を統計データとして表示する．"
      ],
      "metadata": {
        "id": "v5i6dYYtuFey"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W7dPXWlTUCx"
      },
      "source": [
        "Out of this small sample, our optimal estimator mislabeled only a single face (Bush’s\n",
        "face in the bottom row was mislabeled as Blair).\n",
        "We can get a better sense of our estimator's performance using the classification report, which lists recovery statistics label by label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "y_DZ_hP-TUCx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, yfit,\n",
        "                            target_names=faces.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "混同行列にする．"
      ],
      "metadata": {
        "id": "Tqm7RtDZuIst"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvZepd_6TUCx"
      },
      "source": [
        "We might also display the confusion matrix between these classes (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "XSePyeaVTUCx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "mat = confusion_matrix(ytest, yfit)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues',\n",
        "            xticklabels=faces.target_names,\n",
        "            yticklabels=faces.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMoMAJQuTUCx"
      },
      "source": [
        "This helps us get a sense of which labels are likely to be confused by the estimator.\n",
        "\n",
        "For a real-world facial recognition task, in which the photos do not come pre-cropped into nice grids, the only difference in the facial classification scheme is the feature selection: you would need to use a more sophisticated algorithm to find the faces, and extract features that are independent of the pixellation.\n",
        "For this kind of application, one good option is to make use of [OpenCV](http://opencv.org), which, among other things, includes pretrained implementations of state-of-the-art feature extraction tools for images in general and faces in particular."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmkc3_08TUCx"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This has been a brief intuitive introduction to the principles behind support vector machines.\n",
        "These models are a powerful classification method, for a number of reasons:\n",
        "\n",
        "- Their dependence on relatively few support vectors means that they are compact and take up very little memory.\n",
        "- Once the model is trained, the prediction phase is very fast.\n",
        "- Because they are affected only by points near the margin, they work well with high-dimensional data—even data with more dimensions than samples, which is challenging for other algorithms.\n",
        "- Their integration with kernel methods makes them very versatile, able to adapt to many types of data.\n",
        "\n",
        "However, SVMs have several disadvantages as well:\n",
        "\n",
        "- The scaling with the number of samples $N$ is $\\mathcal{O}[N^3]$ at worst, or $\\mathcal{O}[N^2]$ for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.\n",
        "- The results are strongly dependent on a suitable choice for the softening parameter `C`. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\n",
        "- The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation (see the `probability` parameter of `SVC`), but this extra estimation is costly.\n",
        "\n",
        "With those traits in mind, I generally only turn to SVMs once other simpler, faster, and less tuning-intensive methods have been shown to be insufficient for my needs.\n",
        "Nevertheless, if you have the CPU cycles to commit to training and cross-validating an SVM on your data, the method can lead to excellent results."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "jupytext": {
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}