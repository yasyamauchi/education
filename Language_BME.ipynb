{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMM9Mo+eHTD4zKe2nOqm1qw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasyamauchi/education/blob/main/Language_BME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 事前準備  \n"
      ],
      "metadata": {
        "id": "mLWzZV9mjIaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)ToyoNet-ACEからあらかじめダウンロードしていたテキストファイルを，Google Colabの「ファイル」にドラッグする．  \n",
        "この講義では次のファイルを用意している．\n",
        "\n",
        "\n",
        "*   Neko.txt: 夏目漱石「吾輩は猫である」\n",
        "*   yokaidan.txt: 井上円了「妖怪談」\n",
        "  \n",
        "※いずれも[青空文庫](https://www.aozora.gr.jp)より，著作権消滅済み．\n"
      ],
      "metadata": {
        "id": "1RBLip74sCHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)必要なパッケージをインストールする．MeCabのインストールに少し時間がかかる(約1分)．"
      ],
      "metadata": {
        "id": "dnPTgs90lpEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7"
      ],
      "metadata": {
        "id": "XBWKR1Zb80-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-gram  \n",
        "\n",
        "\"Ngram\"は，キーワードの出現頻度を可視化する意味で用いられることが多い．  \n",
        "[Google Ngram Viewer](https://books.google.com/ngrams) (Google Books)  \n",
        "[NDL Ngram Viewer](https://lab.ndl.go.jp/ngramviewer/) (国立国会図書館)  "
      ],
      "metadata": {
        "id": "4JR3zTzR51R1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gramをもとにして文章を生成してみる  \n",
        "参考：[3. Pythonによる自然言語処理　1-1. 単語N-gram](https://qiita.com/y_itoh/items/82222af50bf1f80255eb) @y_itoh(yumi ito)  \n",
        "  \n",
        "この方法は純粋に訓練データ(テキスト)の単語の統計情報から文章を生成するものである．"
      ],
      "metadata": {
        "id": "m1Xto7e9-Afc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colabにダウンロード済みのテキストファイル(**Neko.txt**)を読み込む．  \n",
        "\n",
        "読み込んだテキストが表示される．"
      ],
      "metadata": {
        "id": "wAaSEK4bms_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWHr1b-15pat"
      },
      "outputs": [],
      "source": [
        "with open('Neko.txt', mode='rt', encoding='utf-8') as f:\n",
        "    read_text = f.read()\n",
        "nekotxt = read_text\n",
        "\n",
        "print(nekotxt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "※後で井上円了「妖怪談」にする場合，1行目を次のように修正して再実行すること．  \n",
        "\n",
        "\n",
        "```\n",
        "with open('yokaidan.txt')\n",
        "```"
      ],
      "metadata": {
        "id": "qpPZtqxbvpM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 分かち書き"
      ],
      "metadata": {
        "id": "6iELrjRQtU9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MeCabで分かち書きし，結果を表示する．"
      ],
      "metadata": {
        "id": "2qD7mnlGnUqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import MeCab\n",
        "tagger = MeCab.Tagger(\"-Owakati\")\n",
        "nekotxt = tagger.parse(nekotxt)\n",
        "\n",
        "# print(nekotxt)\n",
        "nekotxt = nekotxt.split()\n",
        "print(nekotxt)"
      ],
      "metadata": {
        "id": "6kfjUYY-9AYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 辞書の作成"
      ],
      "metadata": {
        "id": "RRFCusQjtZCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-gram辞書を生成する．  \n",
        "連続する2語を要素とし，その頻度をカウントしたものがdic2(2-gram)，3語をカウントしたものがdic3(3-gram)である．"
      ],
      "metadata": {
        "id": "QRpKv3pZnZ24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "from numpy.random import *\n",
        "\n",
        "string = nekotxt\n",
        "\n",
        "# 除外する文字記号\n",
        "delimiter = ['「', '」', '…', '　']\n",
        "\n",
        "# 2語のリスト\n",
        "double = list(zip(string[:-1], string[1:]))\n",
        "double = filter((lambda x: not((x[0] in delimiter) or (x[1] in delimiter))), double)\n",
        "\n",
        "# 3語のリスト\n",
        "triple = list(zip(string[:-2], string[1:-1], string[2:]))\n",
        "triple = filter((lambda x: not((x[0] in delimiter) or (x[1] in delimiter) or (x[2] in delimiter))), triple)\n",
        "\n",
        "# 要素数をカウントして辞書を生成\n",
        "dic2 = Counter(double)\n",
        "dic3 = Counter(triple)"
      ],
      "metadata": {
        "id": "BRPof3Oi9PiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-gram辞書を表示する．  \n",
        "長いので中断してしまうが問題ない．  \n",
        "  \n",
        "例えば，\n",
        "\n",
        "```\n",
        "('聞き', 'たまえ') 3\n",
        "```\n",
        "は，「聞き」の直後に「たまえ」があるパターンが3回見つかった，という意味である．\n"
      ],
      "metadata": {
        "id": "WjguFUV-oPIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for u,v in dic2.items():\n",
        "    print(u, v)"
      ],
      "metadata": {
        "id": "Kp9iOV5d9Va5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-gram辞書を表示する．  \n",
        "これも長いので中断してしまうが問題ない．  \n",
        "  \n",
        "同じく，例えば，\n",
        "\n",
        "```\n",
        "('飛び', '上っ', 'て') 2\n",
        "```\n",
        "は，「飛び」「上っ」「て」の連続パターンが2回見つかった，という意味である．\n"
      ],
      "metadata": {
        "id": "tFeg9zUBoRzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for u,v in dic3.items():\n",
        "    print(u, v)"
      ],
      "metadata": {
        "id": "m916dQPk9buC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文章生成"
      ],
      "metadata": {
        "id": "jaN0IqfFtc68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "文章生成の関数を定義する．"
      ],
      "metadata": {
        "id": "mZHLpdPNqq54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nextword(words, dic):\n",
        "    ## ➀先頭の単語wordsの要素数gramsを取得\n",
        "    grams = len(words)\n",
        "\n",
        "    ## ➁N-gram辞書dicから一致する要素を抽出\n",
        "    # 2語の場合\n",
        "    if grams == 2:\n",
        "        matcheditems = np.array(list(filter(\n",
        "            (lambda x: x[0][0] == words[1]), #1番目が合致\n",
        "            dic.items())))\n",
        "    # 3語の場合\n",
        "    else:\n",
        "        matcheditems = np.array(list(filter(\n",
        "            (lambda x: x[0][0] == words[1]) and (lambda x: x[0][1] == words[2]), #1番目と2番目が合致\n",
        "            dic.items())))\n",
        "\n",
        "    ## ➂一致する語がない場合のエラーメッセージ\n",
        "    if(len(matcheditems) == 0):\n",
        "        print(\"No matched generator for\", words[1])\n",
        "        return ''\n",
        "\n",
        "    ## ➃重み付き出現頻度リスト\n",
        "    # matcheditemsから出現頻度を取得\n",
        "    probs = [row[1] for row in matcheditems]\n",
        "    # 0～1の疑似乱数を生成して出現頻度にかける\n",
        "    weightlist = rand(len(matcheditems)) * probs\n",
        "\n",
        "    ## ➄matcheditemsから重み付き出現頻度が最大の要素を取得\n",
        "    if grams == 2:\n",
        "        u = matcheditems[np.argmax(weightlist)][0][1]\n",
        "    else:\n",
        "        u = matcheditems[np.argmax(weightlist)][0][2]\n",
        "    return u"
      ],
      "metadata": {
        "id": "rTT5vknqqpRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "文章生成を試す．  \n",
        "\n",
        "1)まずはそのまま実行すると，「吾輩」からはじまる文章が生成される．これは2-gram辞書を用いている．実行するたびに結果が変わるので，何度か試すとよい．  "
      ],
      "metadata": {
        "id": "7ykUXIyJqRmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 先頭の単語wordsを入力\n",
        "words = ['', '吾輩'] # 2-gram\n",
        "#words = ['', '吾輩', 'は'] # 3-gram\n",
        "\n",
        "# 出力outputの先頭にwordsを埋め込む\n",
        "output = words[1:]\n",
        "\n",
        "# ｢次の語｣を取得\n",
        "for i in range(100):\n",
        "    # 2語の場合\n",
        "    if len(words) == 2:\n",
        "        newword = nextword(words, dic2)\n",
        "    # 3語の場合\n",
        "    else:\n",
        "        newword = nextword(words, dic3)\n",
        "\n",
        "    # 出力outputに次の語を追加\n",
        "    output.append(newword)\n",
        "    # 次の文字が終止符なら終了\n",
        "    if newword in ['', '。', '？', '！']:\n",
        "        break\n",
        "    # 次のnextwordの準備\n",
        "    words = output[-len(words):]\n",
        "    print(words)\n",
        "\n",
        "# 出力outputを表示\n",
        "for u in output:\n",
        "    print(u, end='')\n"
      ],
      "metadata": {
        "id": "0Qd8x6RC9jPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)次に2行目の冒頭に半角の#を書き加え，逆に3行目の冒頭の#を消去して実行する．今度は3-gram辞書を用いている．これも実行のたびに結果が変わる．  \n",
        "\n",
        "```\n",
        "#words = ['', '吾輩'] # 2-gram\n",
        "words = ['', '吾輩', 'は'] # 3-gram\n",
        "```\n",
        "3)最後に好きな単語で文章生成する．たとえば，「吾輩は」の代わりに「猫が」にしてみる．\n",
        "```\n",
        "words = ['', '猫', 'が'] # 3-gram\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "jarpkd7HwH83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 井上円了「妖怪談」を試してみる  \n",
        "  \n",
        "「N-gramをもとにして文章を生成してみる」に戻り，**Neko.txt**を**yokaidan.txt**に書き換えて同じように実行してみる．  \n",
        "最後の文章生成は2-gramで「妖怪」を試してみるとよい(「吾輩」では何も出ない)．  \n",
        "\n",
        "```\n",
        "words = ['', '妖怪'] # 2-gram\n",
        "#words = ['', '吾輩', 'は'] # 3-gram\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cHuYjNONpisq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 文書単語行列と文章類似度  \n",
        "参考：  \n",
        "* [scikit-learnのCountVectorizerやTfidfVectorizerの日本語での使い方について](https://qiita.com/kiyuka/items/3de09e313a75248ca029)　@kiyuka\n",
        "* [Pythonで文書類似度算出！](https://toukei-lab.com/python-mecab)　ウマたん  "
      ],
      "metadata": {
        "id": "PF9Vfl-g5tSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 文書単語行列"
      ],
      "metadata": {
        "id": "MOtBKt-dGY5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次の4つの文章の文書単語行列を求める．あらかじめ分かち書きする．  \n",
        "* これは最初のドキュメントです。\n",
        "* このドキュメントは2番目のドキュメントです。\n",
        "* そして、これは3番目のものです。\n",
        "* これは最初のドキュメントですか?\n",
        "  \n",
        "単に出現頻度が表になっているだけなので難しくはない．"
      ],
      "metadata": {
        "id": "WL6jg4TMGw5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from IPython.display import display\n",
        "corpus = [\n",
        "    ['これ', 'は', '最初', 'の', 'ドキュメント', 'です', '。'],\n",
        "    ['この', 'ドキュメント', 'は', '2', '番目', 'の', 'ドキュメント', 'です', '。'],\n",
        "    ['そして', '、', 'これ', 'は', '3', '番目', 'の', 'もの', 'です', '。'],\n",
        "    ['これ', 'は', '最初', 'の', 'ドキュメント', 'です', 'か', '?']\n",
        "]\n",
        "vectorizer = CountVectorizer(analyzer=lambda x: x)\n",
        "\n",
        "vec = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "df = pd.DataFrame(vec.toarray(), columns=feature_names)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "2BkNLmmPDWPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 二つのURLのテキストの類似度を求める  "
      ],
      "metadata": {
        "id": "CYmJX2XUzuLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### とりあえずやってみる"
      ],
      "metadata": {
        "id": "ij92WcVoHd-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import sys\n",
        "import MeCab\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "##関数定義\n",
        "# Step1：URLからテキスト情報をスクレイピング\n",
        "def geturl(urls):\n",
        "    all_text=[]\n",
        "    for url in urls:\n",
        "        r=requests.get(url)\n",
        "        c=r.content\n",
        "        soup=BeautifulSoup(c,\"html.parser\")\n",
        "        article1_content=soup.find_all(\"p\")\n",
        "        temp=[]\n",
        "        for con in article1_content:\n",
        "            out=con.text\n",
        "            temp.append(out)\n",
        "        text=''.join(temp)\n",
        "        all_text.append(text)\n",
        "        sleep(1)\n",
        "    return all_text\n",
        "\n",
        "# Step2：それらをMeCabで形態素解析。名詞だけ抽出。\n",
        "def mplg(article):\n",
        "    word_list = \"\"\n",
        "    m=MeCab.Tagger()\n",
        "    m1=m.parse (text)\n",
        "    for row in m1.split(\"\\n\"):\n",
        "        word =row.split(\"\\t\")[0]#タブ区切りになっている１つ目を取り出す。ここには形態素が格納されている\n",
        "        if word == \"EOS\":\n",
        "            break\n",
        "        else:\n",
        "            pos = row.split(\"\\t\")[1]#タブ区切りになっている2つ目を取り出す。ここには品詞が格納されている\n",
        "            slice = pos[:2]\n",
        "            if slice == \"名詞\":\n",
        "                word_list = word_list +\" \"+ word\n",
        "    return word_list\n",
        "\n",
        "# Step3：名詞の出現頻度からTF-IDF/COS類似度を算出。テキスト情報のマッチ度を測る\n",
        "def tfidf(word_list):\n",
        "    docs = np.array(word_list)#Numpyの配列に変換する\n",
        "    #単語を配列ベクトル化して、TF-IDFを計算する\n",
        "    vecs = TfidfVectorizer(\n",
        "                token_pattern=u'(?u)\\\\b\\\\w+\\\\b'#文字列長が 1 の単語を処理対象に含めることを意味します。\n",
        "                ).fit_transform(docs)\n",
        "    vecs = vecs.toarray()\n",
        "    return vecs\n",
        "\n",
        "\n",
        "def cossim(v1,v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "##実装\n",
        "word_list=[]\n",
        "texts=geturl([\"https://toukei-lab.com/conjoint\",\"https://toukei-lab.com/correspondence\"])\n",
        "for text in texts:\n",
        "    word_list.append(mplg(text))\n",
        "\n",
        "vecs=tfidf(word_list)\n",
        "print(cossim(vecs[1],vecs[0]))"
      ],
      "metadata": {
        "id": "LeyWjHj5wl-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "最後に数字が1つ表示されたが，これが次の二つのURL(ソースコードでは59行目付近に記載)のコサイン類似度である．  \n",
        "**全く同じ文章の場合，1.0**となる．  \n",
        "* https://toukei-lab.com/conjoint\n",
        "* https://toukei-lab.com/correspondence"
      ],
      "metadata": {
        "id": "Xx5-bVLF0G7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 名詞の抽出"
      ],
      "metadata": {
        "id": "hNJIbLNDHin1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "中身を解説する．MeCabで分かち書きを行うところまでは同じだが，今回は名詞だけを抽出する．結果を見てみる．"
      ],
      "metadata": {
        "id": "6SfWHLKw0kCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('最初のURLの分かち書きの結果(名詞のみ): 単語数{}'.format(len(word_list[0])))\n",
        "print(word_list[0])\n",
        "print('二番目のURLの分かち書きの結果(名詞のみ): 単語数{}'.format(len(word_list[1])))\n",
        "print(word_list[1])"
      ],
      "metadata": {
        "id": "ZznhYHQex7gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文書単語行列の表示"
      ],
      "metadata": {
        "id": "zH_JnrIgHm74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "文書単語行列を表示する．  \n",
        "各々の文章の文書ベクトル(tf-idf)の中身を見ると，二つの文章で異なることがわかる．  \n",
        "もし同じ文章であれば二つのベクトルは全く同じであり，なす角度は0度なので，コサイン類似度cos(0)=1.0となる．"
      ],
      "metadata": {
        "id": "Yai532bIB0Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('ベクトルのサイズ: {}'.format(len(vecs[0])))\n",
        "print('\\n最初のURLの名詞のtf-idf')\n",
        "print(vecs[0])\n",
        "print('\\n二番目のURLのの名詞のtf-idf')\n",
        "print(vecs[1])"
      ],
      "metadata": {
        "id": "F2GhJFWd_QF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "idfの計算方法は複数あり，上記のscikit-learnのTfidfVectorizerはデフォルトで次の式となっている．  \n",
        "$$idf=\\ln \\frac{すべてのテキスト数+1}{その単語を含むテキスト数+1}+1$$\n",
        "東大スライドとの違いは，分母分子に＋１があること，対数が自然対数(ネイビア数が底)である．  \n",
        "これにtf(単語の頻度)を乗じたものがtf-idfであるが，TfidfVectorizerではtf-idfは最終的にはベクトルごとに正規化(二乗和が1)しているので，どれも1未満である．  \n",
        "  \n",
        "  参考：  \n",
        "  * [Scikit learn: 6.2 Feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
        "  * [Wikipedia: tf-idf](https://ja.wikipedia.org/wiki/Tf-idf)"
      ],
      "metadata": {
        "id": "Nxs8wgjtIBD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 単語の分散表現とword2vec"
      ],
      "metadata": {
        "id": "q499YWs1RWn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vecは2010年に当時Googleに在籍していたT. Mikolovらにより開発されたシンプルなRNNであり，次のような特徴を持つ．  \n",
        "* 単語は多次元ベクトルで表現される(これを分散表現と呼ぶ)\n",
        "* 似た単語は似たベクトルを有する  \n",
        "  \n",
        "有名な例として\"King - man + woman = queen\"が挙げられることが多い．manからkingへのベクトルと，womanからqueenへのベクトルが等しい．  \n",
        "  \n",
        "参考：  \n",
        "* [word2vecをColab環境で使うための5行](https://qiita.com/Ninagawa123/items/6c38160e041b6c333905) @Ninagawa123(Izumi Ninagawa) ※word2vecの旧バージョンなのでそのままでは動かない"
      ],
      "metadata": {
        "id": "kJWGJPGkR1RZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wikipediaから抽出したコーパスの準備"
      ],
      "metadata": {
        "id": "1hwd-gNzo04T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず\"text8\"というコーパスを読み込む．これはWikipediaから抽出した100MBほどの前処理済みデータセットである．    \n",
        "参考：\n",
        "* [text8](https://mattmahoney.net/dc/textdata.html)"
      ],
      "metadata": {
        "id": "aEI0jNsFh_os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as gendl\n",
        "corpus = gendl.load(\"text8\")"
      ],
      "metadata": {
        "id": "6_qpWpZOhI3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### word2vecによる学習"
      ],
      "metadata": {
        "id": "d7MKH-W0o6Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次にword2vecを用いて学習させ，ベクトル化する．しばらく時間がかかる(約7分)．  \n",
        "[gensim](https://radimrehurek.com/gensim/)は自然言語処理のライブラリ．"
      ],
      "metadata": {
        "id": "iuQsRf8ujsuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim #ライブラリgensimを導入する\n",
        "model = gensim.models.Word2Vec(corpus, vector_size=200, window=5, epochs=10, min_count=1)"
      ],
      "metadata": {
        "id": "9_rwjySSFq5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習結果"
      ],
      "metadata": {
        "id": "olHjd3UYo9xD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "では，\"coffee\"に近い単語を表示してみる．表示される数字は，文章類似度で取り上げたコサイン類似度である(つまり全く同じ単語で1.0)．"
      ],
      "metadata": {
        "id": "fiZKpGD0j3FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('coffee')"
      ],
      "metadata": {
        "id": "BIIRFDFkhmd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**coffee**の部分を各自で書き換えて実行してみよう．  \n",
        "ただし，固有名詞ではうまくいかない(例：Japan)．また，コーパスが英語なので日本語は認識しない．"
      ],
      "metadata": {
        "id": "EE8o0VR3kcqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"coffee\"のベクトルの中身を見てみる．学習の時にベクトルサイズを200で設定したので，このデータは200次元のベクトルデータとなる．"
      ],
      "metadata": {
        "id": "aVs8EFKWmfSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv['coffee'])"
      ],
      "metadata": {
        "id": "PV20oaI_kysM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vecは\"coffee\"に一番近い単語としてbananaを挙げたが，そのbananaのベクトルも見てみる．先ほどのcoffeeと似てるだろうか？"
      ],
      "metadata": {
        "id": "qLql42DEm8Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv['banana'])"
      ],
      "metadata": {
        "id": "pf9W3bo7mdK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 単語間の演算  \n"
      ],
      "metadata": {
        "id": "xBEOZq1HothG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "「女王」＋「男」－「女」を計算してみよう！"
      ],
      "metadata": {
        "id": "XxplYOXhnvMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=['queen', 'man'], negative=['woman'])"
      ],
      "metadata": {
        "id": "8j2TYGc5m30p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "「コーヒー」＋「牛乳」は？"
      ],
      "metadata": {
        "id": "gNGojcf6olyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=['coffee','milk'])"
      ],
      "metadata": {
        "id": "3GIGsRPTn_t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "「ケーキ」＋「塩」－「砂糖」は？"
      ],
      "metadata": {
        "id": "6EUqtZE0oF4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=['cake', 'salt'], negative=['sugar'])"
      ],
      "metadata": {
        "id": "vuNeRiysn7to"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}